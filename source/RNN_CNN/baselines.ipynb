{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import utils\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima_model import ARIMA, ARMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n",
    "from statsmodels.tsa.api import VAR\n",
    "import statsmodels as sm\n",
    "from settings import in_size, out_size, train_start, train_end, valid_start, valid_end, test_start, test_end, interval\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import pandas.tseries.offsets as offsets\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num = 2000 #ARIMA, SARIMA モデルの訓練に使うデータ数．0 にすれば全訓練データを使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = pickle.load(open(\"./data/data.pickle\",\"rb\"))\n",
    "cluster_sizes = _[\"cluster_sizes\"]\n",
    "cluster2id = _[\"cluster2id\"]\n",
    "locations = _[\"locations\"]\n",
    "timestamps = _[\"timestamps\"]\n",
    "raw_data = _[\"raw_data\"]\n",
    "cluster_data = _[\"cluster_data\"]\n",
    "onehot_data = _[\"onehot_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(cluster_data * cluster_sizes,index=timestamps,columns=['cluster_'+str(i) for i in range(len(cluster_sizes))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.iloc[train_start:valid_end].asfreq(\"15T\").interpolate()\n",
    "#valid_data = data.iloc[valid_start:valid_end].asfreq(\"15T\").interpolate()\n",
    "test_data = data.iloc[test_start:test_end].asfreq(\"15T\").interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#全駐車場のデータ\n",
    "df_sum = train_data.sum(axis=1)\n",
    "df_sum[-2000:].plot(figsize=(16,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1日周期としたとき\n",
    "sm.tsa.seasonal.seasonal_decompose(df_sum[-2000:], freq=96).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_arima(train_data):\n",
    "    #和分過程っぽさはないので ARMA として訓練（d=0）\n",
    "    result_tuning = sm.tsa.stattools.arma_order_select_ic(train_data[-train_num:],max_ar=3,max_ma=3)\n",
    "    p,q = result_tuning[\"bic_min_order\"]\n",
    "    arima_results = ARIMA(\n",
    "        train_data[-train_num:],freq=\"15T\",order=(p,0,q), \n",
    "    ).fit(maxiter=500,disp=False)\n",
    "    return arima_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sarimax(train_data):\n",
    "    #和分過程っぽさはないので ARMA として訓練（d=0）\n",
    "    result_tuning = sm.tsa.stattools.arma_order_select_ic(train_data[-train_num:],max_ar=3,max_ma=3)\n",
    "    p,q = result_tuning[\"bic_min_order\"]\n",
    "    sarima_results = SARIMAX(\n",
    "        train_data[-train_num:],freq=\"15T\",order=(p,0,q),seasonal_order=(0,1,0,96),\n",
    "        enforce_invertibility=False,enforce_stationarity=False\n",
    "    ).fit(maxiter=500)\n",
    "    return sarima_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_arima(model, test_data):\n",
    "    mae_cluster = np.zeros(out_size)\n",
    "    y_preds = model.predict(model.fittedvalues.index[-1], test_data.index[-1])\n",
    "    test_steps = 0\n",
    "    plt.show()\n",
    "    y_pred_matrix = []\n",
    "    y_ref_matrix = []\n",
    "    \n",
    "    for i in tqdm_notebook(range(0,len(test_data)-in_size-out_size,out_size)):\n",
    "        test_steps += 1\n",
    "        y_ref = test_data.iloc[i+in_size:in_size+i+out_size]\n",
    "        y_pred = y_preds[y_ref.index]\n",
    "        mae_cluster += np.abs(y_ref.values - y_pred.values)\n",
    "        y_pred_matrix.append(y_pred.values)\n",
    "        y_ref_matrix.append(y_ref.values)\n",
    "    mae_cluster /= test_steps\n",
    "    return mae_cluster, np.array(y_pred_matrix), np.array(y_ref_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_arima(train_data,test_data,seasonal=False):\n",
    "    results = []\n",
    "    y_pred_sum = []\n",
    "    y_ref_sum = []\n",
    "    \n",
    "    for i,items in enumerate(train_data.iteritems()):\n",
    "        print(\"garvage collection,\",gc.collect())\n",
    "        c_num,cluster_train_data = items\n",
    "        print(\"------cluster id\",c_num,\"最大駐車台数\",cluster_sizes[i],\"台------\")\n",
    "        print(\"平均絶対値誤差 (正解 - 予測) [台]\")\n",
    "        print(\"training...\",end=\"\")\n",
    "        cluster_test_data = test_data[c_num]\n",
    "        \n",
    "        #train phase\n",
    "        if not seasonal:\n",
    "            model = create_arima(cluster_train_data)\n",
    "        else:\n",
    "            model = create_sarimax(cluster_train_data)\n",
    "        print(\"done.\")\n",
    "        \n",
    "        #test phase\n",
    "        print(\"test_\"+c_num)\n",
    "        mae_cluster, y_pred_matrix, y_ref_matrix = eval_arima(model, cluster_test_data)\n",
    "        results.append(mae_cluster)\n",
    "        for j, mae in enumerate(mae_cluster):\n",
    "            print(str(j * interval)+\"分後: \",mae)\n",
    "        y_pred_sum.append(y_pred_matrix)\n",
    "        y_ref_sum.append(y_ref_matrix)\n",
    "        \n",
    "    print(\"--------total       最大駐車台数\",sum(cluster_sizes),\"台--------\")\n",
    "    y_pred_sum = sum(y_pred_sum)\n",
    "    y_ref_sum = sum(y_ref_sum)\n",
    "    mae_total = np.mean(np.abs(y_ref_sum - y_pred_sum),axis=0) #全体での平均絶対値誤差\n",
    "    results.append(mae_total)\n",
    "    for j, mae in enumerate(mae_total):\n",
    "        print(str(j * interval)+\"分後: \",mae)\n",
    "        \n",
    "    df_results = pd.DataFrame(results,\n",
    "                 columns=[str((i+1) * interval)+\"分後\" for i in range(out_size)],\n",
    "                 index = [str(idx)+\" (\"+str(cluster_sizes[idx])+\"台)\" for idx in range(len(cluster_sizes))] + [\"total\"]\n",
    "                )[[\"15分後\",\"30分後\",\"60分後\",\"120分後\"]]\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_arima = run_arima(train_data,test_data)\n",
    "df_results_arima.to_csv(\"./results/csv/arima_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_results_sarima = run_arima(train_data,test_data,seasonal=True)\n",
    "df_results_sarima.to_csv(\"./results/csv/sarima_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_sarima"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
