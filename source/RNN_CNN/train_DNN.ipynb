{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import Model\n",
    "import utils\n",
    "import pickle\n",
    "import numpy as np\n",
    "import keras\n",
    "from importlib import reload\n",
    "from settings import in_size, out_size, train_start, train_end, valid_start, valid_end\n",
    "from utils.Model import create_gnn_seq2seq\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model.set_gpu(\"7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = pickle.load(open(\"./data/data.pickle\",\"rb\"))\n",
    "cluster_sizes = _[\"cluster_sizes\"]\n",
    "cluster2id = _[\"cluster2id\"]\n",
    "locations = _[\"locations\"]\n",
    "timestamps = _[\"timestamps\"]\n",
    "raw_data = _[\"raw_data\"]\n",
    "cluster_data = _[\"cluster_data\"]\n",
    "onehot_data = _[\"onehot_data\"]\n",
    "half = 77042\n",
    "one_year = 59570\n",
    "one_year_and_half = 42098\n",
    "two_years = 24626\n",
    "full = 8500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full = utils.data_iterator(cluster_data[train_start:train_end],timestamps[train_start:train_end],in_size,out_size=out_size,slide=1)\n",
    "train_half = utils.data_iterator(cluster_data[half:train_end],timestamps[half:train_end],in_size,out_size=out_size,slide=1)\n",
    "train_one = utils.data_iterator(cluster_data[one_year:train_end],timestamps[one_year:train_end],in_size,out_size=out_size,slide=1)\n",
    "train_one_half = utils.data_iterator(cluster_data[one_year_and_half:train_end],timestamps[one_year_and_half:train_end],in_size,out_size=out_size,slide=1)\n",
    "train_two = utils.data_iterator(cluster_data[two_years:train_end],timestamps[two_years:train_end],in_size,out_size=out_size,slide=1)\n",
    "\n",
    "valid = utils.data_iterator(cluster_data[valid_start:valid_end],timestamps[valid_start:valid_end],in_size,out_size=out_size,slide=1)\n",
    "train_onehot = utils.data_generator(onehot_data[train_start:train_end],timestamps[train_start:train_end],in_size,out_size=out_size,slide=1)\n",
    "valid_onehot = utils.data_iterator(onehot_data[valid_start:valid_end],timestamps[valid_start:valid_end],in_size,out_size=out_size,slide=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callbacks(modelname):\n",
    "    # callbacks を生成する関数\n",
    "    \n",
    "    # valid で最も良い性能になったモデルを保存する\n",
    "    mc_cb = keras.callbacks.ModelCheckpoint(\n",
    "        './models/'+modelname+'.clf', \n",
    "        monitor='val_loss', \n",
    "        verbose=1, \n",
    "        save_best_only=True,\n",
    "        save_weights_only=False, \n",
    "        mode='auto', \n",
    "        period=1\n",
    "    )\n",
    "\n",
    "    # val_loss が7エポック以上減少しなかったら訓練終了\n",
    "    es_cb = keras.callbacks.EarlyStopping(monitor='val_loss', patience=7, verbose=1, mode='auto')\n",
    "    \n",
    "    # 2エポック以上 val_loss が減少しなかったら学習率を削減\n",
    "    rl_cb = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', \n",
    "        factor=0.3, \n",
    "        patience=2, \n",
    "        verbose=1, \n",
    "        mode='auto', \n",
    "        epsilon=0.00005, \n",
    "        cooldown=2, \n",
    "        min_lr=0\n",
    "    )\n",
    "    return [mc_cb,rl_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "半年\n",
    "s2s = Model.create_seq2seq(len(cluster2id),attr_size=4,pad_size=in_size,out_size=out_size,loss='mae')\n",
    "history = s2s.fit(\n",
    "    train_half[0],train_half[1],batch_size=512,epochs=100,\n",
    "    validation_data=valid,shuffle=True,callbacks=callbacks('s2s_mae_half')\n",
    ")\n",
    "\n",
    "conv_s2s = Model.create_conv_encoder_seq2seq(len(cluster2id),attr_size=4,pad_size=in_size,out_size=out_size,loss='mae',lr=0.0001,latent_dim=128)\n",
    "history = conv_s2s.fit(\n",
    "    train_half[0],train_half[1],batch_size=512,epochs=100,\n",
    "    validation_data=valid,shuffle=True,callbacks=callbacks('s2s_conv_mae_half')\n",
    ")\n",
    "\n",
    "create_gnn_seq2seq.fit(\"half\", half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1年\n",
    "s2s = Model.create_seq2seq(len(cluster2id),attr_size=4,pad_size=in_size,out_size=out_size,loss='mae')\n",
    "history = s2s.fit(\n",
    "    train_one[0],train_one[1],batch_size=512,epochs=100,\n",
    "    validation_data=valid,shuffle=True,callbacks=callbacks('s2s_mae_one')\n",
    ")\n",
    "\n",
    "conv_s2s = Model.create_conv_encoder_seq2seq(len(cluster2id),attr_size=4,pad_size=in_size,out_size=out_size,loss='mae')\n",
    "history = conv_s2s.fit(\n",
    "    train_one[0],train_one[1],batch_size=512,epochs=100,\n",
    "    validation_data=valid,shuffle=True,callbacks=callbacks('s2s_conv_mae_one')\n",
    ")\n",
    "\n",
    "create_gnn_seq2seq.fit(\"one\", one_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1年半\n",
    "s2s = Model.create_seq2seq(len(cluster2id),attr_size=4,pad_size=in_size,out_size=out_size,loss='mae')\n",
    "history = s2s.fit(\n",
    "    train_one_half[0],train_one_half[1],batch_size=512,epochs=100,\n",
    "    validation_data=valid,shuffle=True,callbacks=callbacks('s2s_mae_one_half')\n",
    ")\n",
    "\n",
    "conv_s2s = Model.create_conv_encoder_seq2seq(len(cluster2id),attr_size=4,pad_size=in_size,out_size=out_size,loss='mae')\n",
    "history = conv_s2s.fit(\n",
    "    train_one_half[0],train_one_half[1],batch_size=512,epochs=100,\n",
    "    validation_data=valid,shuffle=True,callbacks=callbacks('s2s_conv_mae_one_half')\n",
    ")\n",
    "\n",
    "create_gnn_seq2seq.fit(\"one_half\", one_year_and_half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2年\n",
    "s2s = Model.create_seq2seq(len(cluster2id),attr_size=4,pad_size=in_size,out_size=out_size,loss='mae')\n",
    "history = s2s.fit(\n",
    "    train_two[0],train_two[1],batch_size=512,epochs=100,\n",
    "    validation_data=valid,shuffle=True,callbacks=callbacks('s2s_mae_two')\n",
    ")\n",
    "\n",
    "conv_s2s = Model.create_conv_encoder_seq2seq(len(cluster2id),attr_size=4,pad_size=in_size,out_size=out_size,loss='mae')\n",
    "history = conv_s2s.fit(\n",
    "    train_two[0],train_two[1],batch_size=512,epochs=100,\n",
    "    validation_data=valid,shuffle=True,callbacks=callbacks('s2s_conv_mae_two')\n",
    ")\n",
    "\n",
    "create_gnn_seq2seq.fit(\"two\", two_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full\n",
    "s2s = Model.create_seq2seq(len(cluster2id),attr_size=4,pad_size=in_size,out_size=out_size,loss='mae')\n",
    "history = s2s.fit(\n",
    "    train_full[0],train_full[1],batch_size=512,epochs=100,\n",
    "    validation_data=valid,shuffle=True,callbacks=callbacks('s2s_mae')\n",
    ")\n",
    "\n",
    "conv_s2s = Model.create_conv_encoder_seq2seq(len(cluster2id),attr_size=4,pad_size=in_size,out_size=out_size,loss='mae')\n",
    "history = conv_s2s.fit(\n",
    "    train_full[0],train_full[1],batch_size=512,epochs=100,\n",
    "    validation_data=valid,shuffle=True,callbacks=callbacks('s2s_conv_mae')\n",
    ")\n",
    "\n",
    "create_gnn_seq2seq.fit(\"full\", full)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}